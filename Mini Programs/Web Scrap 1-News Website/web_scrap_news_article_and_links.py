# -*- coding: utf-8 -*-
"""Web_Scrap_News_Article_and_Links.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dUwdVGBYSC0Y5GDisJ_4QCUy4ZEnNADE
"""

#import the libraries
import requests
from bs4 import BeautifulSoup
import pandas as pd

#use the get method from requests to access the website
results=requests.get("https://www.whitehouse.gov/briefing-room/")

#identify the status
results.status_code

#get the header, not needed, optional; results.content also content
results.headers

#create a BeautifulSoup object, then find_all the h2 or links, etc.
soup=BeautifulSoup(results.text,"lxml")
article_links=soup.find_all('a', class_="news-item__title")
article_links

#create an emtpy list (articles), then for each link in articles_links, retrieve the text and strip to remove extra white spaces from beginning, then get the href link
#lastly, append to articles
articles=[]
for link in article_links:
    print(link.text.strip())
    print(link.get('href').strip())
    articles.append((link.text.strip(),link.get('href'))) #append a tuple rather than append (x,y) since append only takes 1 argument

"""
Alternative way and sample code to get the links
```
urls = []

for h2_tag in soup.find_all("h2"):
    a_tag = h2_tag.find("a")  # Find a single <a> element inside <h2>
    if a_tag:  # Check if a_tag is not None
        urls.append(a_tag.attrs.get("href"))  # Get the 'href' attribute and append it to urls

print(urls)  # Output the list of URLs

```

"""

#put the article title and links to a pandas table
article_table=pd.DataFrame(articles,columns=['Article title','Article link'])
article_table





